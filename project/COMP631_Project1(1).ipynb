{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"osm4Zmm4KS5w","outputId":"76664ea4-f641-47c5-f836-5068a83a0b9b","executionInfo":{"status":"ok","timestamp":1644804613780,"user_tz":360,"elapsed":23459,"user":{"displayName":"Yuan Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBJ0riEbMZ4_ksWQUJjjNFSi7ruMJ8UYx9uTtT=s64","userId":"11796422207038387991"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scrapy\n","  Downloading Scrapy-2.5.1-py2.py3-none-any.whl (254 kB)\n","\u001b[?25l\r\u001b[K     |█▎                              | 10 kB 20.8 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 20 kB 22.4 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 30 kB 19.9 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 40 kB 12.9 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 51 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 61 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 71 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 81 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 92 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 102 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 112 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 122 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 133 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 143 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 153 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 163 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 174 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 184 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 194 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 204 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 215 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 225 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 235 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 245 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 254 kB 12.5 MB/s \n","\u001b[?25hCollecting itemloaders>=1.0.1\n","  Downloading itemloaders-1.0.4-py3-none-any.whl (11 kB)\n","Collecting itemadapter>=0.1.0\n","  Downloading itemadapter-0.4.0-py3-none-any.whl (10 kB)\n","Collecting pyOpenSSL>=16.2.0\n","  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n","\u001b[K     |████████████████████████████████| 55 kB 3.7 MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (4.2.6)\n","Collecting cssselect>=0.9.1\n","  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n","Collecting zope.interface>=4.1.3\n","  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n","\u001b[K     |████████████████████████████████| 251 kB 40.6 MB/s \n","\u001b[?25hCollecting queuelib>=1.4.2\n","  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n","Collecting protego>=0.1.15\n","  Downloading Protego-0.2.0-py2.py3-none-any.whl (8.2 kB)\n","Collecting service-identity>=16.0.0\n","  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n","Collecting PyDispatcher>=2.0.5\n","  Downloading PyDispatcher-2.0.5.zip (47 kB)\n","\u001b[K     |████████████████████████████████| 47 kB 2.3 MB/s \n","\u001b[?25hCollecting parsel>=1.5.0\n","  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n","Collecting w3lib>=1.17.0\n","  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n","Collecting Twisted[http2]>=17.9.0\n","  Downloading Twisted-22.1.0-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 35.4 MB/s \n","\u001b[?25hCollecting h2<4.0,>=3.0\n","  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n","\u001b[K     |████████████████████████████████| 65 kB 3.6 MB/s \n","\u001b[?25hCollecting cryptography>=2.0\n","  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n","\u001b[K     |████████████████████████████████| 3.6 MB 65.9 MB/s \n","\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.0->scrapy) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.0->scrapy) (2.21)\n","Collecting hpack<4,>=3.0\n","  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n","Collecting hyperframe<6,>=5.2.0\n","  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n","Collecting jmespath>=0.9.5\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Requirement already satisfied: six>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from parsel>=1.5.0->scrapy) (1.15.0)\n","Requirement already satisfied: pyasn1 in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n","Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy) (21.4.0)\n","Collecting constantly>=15.1\n","  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n","Collecting incremental>=21.3.0\n","  Downloading incremental-21.3.0-py2.py3-none-any.whl (15 kB)\n","Collecting hyperlink>=17.1.1\n","  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n","\u001b[K     |████████████████████████████████| 74 kB 3.6 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted[http2]>=17.9.0->scrapy) (3.10.0.2)\n","Collecting Automat>=0.8.0\n","  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n","Collecting priority<2.0,>=1.1.0\n","  Downloading priority-1.3.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted[http2]>=17.9.0->scrapy) (2.10)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zope.interface>=4.1.3->scrapy) (57.4.0)\n","Building wheels for collected packages: PyDispatcher\n","  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=11516 sha256=42ab29c52a8a6c2dfaef7286c04ff5be6f40f9be0bb90824f72de0a8d3de7c56\n","  Stored in directory: /root/.cache/pip/wheels/2d/18/21/3c6a732eaa69a339198e08bb63b7da2c45933a3428b29ec454\n","Successfully built PyDispatcher\n","Installing collected packages: zope.interface, w3lib, incremental, hyperlink, hyperframe, hpack, cssselect, constantly, Automat, Twisted, priority, parsel, jmespath, itemadapter, h2, cryptography, service-identity, queuelib, pyOpenSSL, PyDispatcher, protego, itemloaders, scrapy\n","Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 Twisted-22.1.0 constantly-15.1.0 cryptography-36.0.1 cssselect-1.1.0 h2-3.2.0 hpack-3.0.0 hyperframe-5.2.0 hyperlink-21.0.0 incremental-21.3.0 itemadapter-0.4.0 itemloaders-1.0.4 jmespath-0.10.0 parsel-1.6.0 priority-1.3.0 protego-0.2.0 pyOpenSSL-22.0.0 queuelib-1.6.2 scrapy-2.5.1 service-identity-21.1.0 w3lib-1.22.0 zope.interface-5.4.0\n","Collecting praw\n","  Downloading praw-7.5.0-py3-none-any.whl (176 kB)\n","\u001b[K     |████████████████████████████████| 176 kB 14.1 MB/s \n","\u001b[?25hCollecting prawcore<3,>=2.1\n","  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\n","Collecting websocket-client>=0.54.0\n","  Downloading websocket_client-1.2.3-py3-none-any.whl (53 kB)\n","\u001b[K     |████████████████████████████████| 53 kB 2.2 MB/s \n","\u001b[?25hCollecting update-checker>=0.18\n","  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n","Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from prawcore<3,>=2.1->praw) (2.23.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2021.10.8)\n","Installing collected packages: websocket-client, update-checker, prawcore, praw\n","Successfully installed praw-7.5.0 prawcore-2.3.0 update-checker-0.18.0 websocket-client-1.2.3\n","\u001b[31mERROR: Could not find a version that satisfies the requirement urlparse (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for urlparse\u001b[0m\n","Collecting pymysql\n","  Downloading PyMySQL-1.0.2-py3-none-any.whl (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 2.0 MB/s \n","\u001b[?25hInstalling collected packages: pymysql\n","Successfully installed pymysql-1.0.2\n"]}],"source":["%pip install scrapy\n","%pip install praw\n","%pip install urlparse\n","%pip install pymysql"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"LnIq0aJpKTz5","outputId":"ec4ce04d-52a7-457c-8d41-b9d3abbbf84f","executionInfo":{"status":"ok","timestamp":1644804621425,"user_tz":360,"elapsed":888,"user":{"displayName":"Yuan Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBJ0riEbMZ4_ksWQUJjjNFSi7ruMJ8UYx9uTtT=s64","userId":"11796422207038387991"}}},"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'116991958771-_TADy3X589-DKbF57KzkAgU7miYDbQ'"]},"metadata":{},"execution_count":2}],"source":["import requests\n","import requests.auth\n","\n","client_auth = requests.auth.HTTPBasicAuth('PkK2HAiKXWi4lVmek7mu9g','G-vjkM8dq1NuZjwizkEd3UMLgHfhfg')\n","post_data = {\n","      \"grant_type\": \"password\", \"username\": \"cysbc1999\", \"password\": \"cysbc1999\"}\n","headers = {\n","      \"User-Agent\": \"ChangeMeClient/0.1 by YourUsername\"}\n","response = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth, data=post_data, headers=headers)\n","\n","authToken = response.json()\n","authToken[\"access_token\"]\n","# print(response.json())"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"btYClf2qPLki","executionInfo":{"status":"ok","timestamp":1644804626886,"user_tz":360,"elapsed":880,"user":{"displayName":"Yuan Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBJ0riEbMZ4_ksWQUJjjNFSi7ruMJ8UYx9uTtT=s64","userId":"11796422207038387991"}}},"outputs":[],"source":["import scrapy\n","import praw\n","import datetime\n","\n","class RedditSpider(scrapy.Spider):\n","\tname = \"reddit\"\n","\tallowed_domains = [\"reddit.com\"]\n","\tstart_urls = [\n","\t\t\"https://www.reddit.com\"\n","\t]\n","\n","\tdef parse(self, response):\n","        #使用client id 和secret 进行登陆\n","\t\treddit = praw.Reddit(client_id='PkK2HAiKXWi4lVmek7mu9g', client_secret='G-vjkM8dq1NuZjwizkEd3UMLgHfhfg',\n","\t\t\tgrant_type='client_credentials', user_agent='mytestscripts/1.0')\n","\t\t\n","\t\t\"\"\"\n","\t\tsub = reddit.submission(id='9klf7s')\n","\t\t#print(sub.title)\n","\t\t#pprint.pprint(vars(sub))\n","\t\t\"\"\"\n","\t\t#可以通过 subreddit.stream.submissions()来监控某一个子版块出现的新帖子\n","\t\t#subreddit = reddit.subreddit('dapps')\n","\t\t#for sub in subreddit.stream.submissions():\n","        \n","        #limit=None来获取所有的贴子，默认为100\n","        #每次得到的属性类别数量可能不一样\n","\t\tsubs = reddit.subreddit('dapps').new(limit=None)\n","\t\tfor sub in subs:\n","\t\t\titem = RedditItem()\n","\n","\t\t\titem['html'] = response.body\n","\t\t\t#print(item['html'])\n","\n","            #permalink是网站下该帖子的前缀，需要和网站地址拼接构成该帖子的链接地址\n","\t\t\turl = 'https://{}{}'.format(self.allowed_domains[0], sub.permalink)\n","\t\t\titem['url'] = url\n","\t\t\tredditor = sub.author\n","            #作者可能为空\n","\t\t\t#print(\"author:\", redditor.name)\n","\t\t\tif redditor is not None:\n","\t\t\t\titem['author'] = redditor.name\n","\t\t\telse:\n","\t\t\t\titem['author'] = \"\"\n","\n","            #sub.created_utc是一个utc时间戳，需要转换成datetime格式\n","\t\t\t#print(\"created utc:\", sub.created_utc)\n","\t\t\titem['created_time'] = datetime.datetime.utcfromtimestamp(sub.created_utc)\n","\n","            #如果帖子本身只是一个超链接，那么sub.selftext为空\n","\t\t\titem['selftext'] = sub.selftext\n","\t\t\tif sub.is_self==False :\n","\t\t\t\titem['selftext'] = sub.url\n","\n","\t\t\tyield item\n","\n","\n","html_insert = '''insert into reddit_dapps_html(html) values('{html}')'''\n","reddit_insert = '''insert into reddit_dapps(url, url_md5, title, author, \n","created_time, selftext, score, num_comments, upvote_ratio)\n","          values('{url}', '{url_md5}', '{title}', '{author}', \n","          '{created_time}', '{selftext}', '{score}', '{num_comments}', '{upvote_ratio}')'''\n","\n","def process_item(self, item, spider):\n","  html = item['html']\n","  if html:\n","    item['html'] = html.strip().decode(encoding=\"utf-8\")\n","\n","\n","  #将时间格式化\n","  #created_time\n","  created_time = item['created_time']\n","  if created_time:\n","    item['created_time'] = created_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n","\n","  selftext = item['selftext']\n","  if selftext:\n","    item['selftext'] = selftext.replace('\\n', '').replace('  ', ' ')\n","\n","  sqltext1 = self.html_insert.format(\n","    html = pymysql.escape_string(item['html']))\n","\n","  #由于score等是数字，需要先转换为字符串格式\n","  sqltext2 = self.reddit_insert.format(\n","    url = pymysql.escape_string(item['url']),\n","    score = pymysql.escape_string(str(item['score'])),\n","    num_comments = pymysql.escape_string(str(item['num_comments'])),\n","    upvote_ratio = pymysql.escape_string(str(item['upvote_ratio'])))\n","  self.cursor.execute(sqltext1)\n","  self.cursor.execute(sqltext2)\n","\n","  return item\n","\n","\n","\n","\n","def open_spider(self, spider):\n","    jdbc = \"mysql://bb35b85be45105:bb27a820@us-cdbr-east-05.cleardb.net/heroku_9576b3861dcc914?reconnect=true\"\n","    jdbc_pattern = 'mysql://(.*?):(\\d*)/'\n","    result=  urlparse(jdbc)\n","    # connet database\n","      # 选择字符集为'utf8mb4'\n","    self.connect = pymysql.connect(\n","        host=\"us-cdbr-east-05.cleardb.net/heroku_9576b3861dcc914\",\n","        user=\"bb35b85be45105\",\n","        passwd=\"bb27a820\",\n","        charset='utf8mb4',\n","        use_unicode=True)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"DxDFtQi6SloO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644804719180,"user_tz":360,"elapsed":421,"user":{"displayName":"Yuan Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBJ0riEbMZ4_ksWQUJjjNFSi7ruMJ8UYx9uTtT=s64","userId":"11796422207038387991"}},"outputId":"9c9c3d93-b67f-4972-ad75-d579edc02537"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'Authorization': '116991958771-_TADy3X589-DKbF57KzkAgU7miYDbQ'}\n","429\n","null\n"]}],"source":["from datetime import datetime\n","\n","slice_headers = {}\n","slice_headers['Authorization'] = authToken[\"access_token\"]\n","\n","print(slice_headers)\n","\t\t\n","params = {\n","'limit':'1'}\t#限制一次取得的数量\n","count = 1\n","while count<3:\n","    response = requests.get(\"https://reddit.com/r/dapps/new.json\",headers = slice_headers,  params=params)\n","\n","    print(response.status_code)\n","\n","    #需要判断response的状态码\n","    if response.status_code==200:\n","        response_json = response.json()\n","        # print(response_json)\n","\n","        now = datetime.now() # current date and time\n","\n","        year = now.strftime(\"%Y\")\n","        print(\"year:\", year)\n","\n","        month = now.strftime(\"%m\")\n","        print(\"month:\", month)\n","\n","        day = now.strftime(\"%d\")\n","        print(\"day:\", day)\n","\n","        time = now.strftime(\"%H:%M:%S\")\n","        print(\"time:\", time)\n","\n","        date_time = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n","        print(\"date and time:\",date_time)\n","\n","        f = open(date_time+\".txt\", \"a\")\n","        f.write(\"Now the file has more content!\")\n","        f.close()\n","\n","        \n","\n","\n","        after = response_json['data']['after']\n","        if after==None:\n","            break\n","        params = {\n","'limit':'1', 'after':after}\n","\n","    else:\n","        print(\"null\")\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yL9nLSPP_iil","outputId":"848bd452-b1e6-46a3-ca31-48f84ca39839"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'Authorization': '116991958771-TvC0N_yz-HZdrAEDUBVTipNEQvACdA'}\n"]}],"source":["slice_headers = {}\n","slice_headers['Authorization'] = authToken[\"access_token\"]\n","\n","print(slice_headers)"]},{"cell_type":"code","source":["reddit_read_only = praw.Reddit(client_id=\"\",         # your client id\n","                               client_secret=\"\",      # your client secret\n","                               user_agent=\"\")        # your user agent\n"," \n"," \n","subreddit = reddit_read_only.subreddit(\"redditdev\")\n"," \n","# Display the name of the Subreddit\n","print(\"Display Name:\", subreddit.display_name)\n"," \n","# Display the title of the Subreddit\n","print(\"Title:\", subreddit.title)\n"," \n","# Display the description of the Subreddit\n","print(\"Description:\", subreddit.description)"],"metadata":{"id":"ZHBiHZteG-5h"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"COMP631_Project1.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}
