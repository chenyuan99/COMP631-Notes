{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"COMP631_Project1.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1zETz8w1G1ZkRfsxlQhxpnxdR6UuhYfo2","authorship_tag":"ABX9TyP3meec+MJrexX1AYm+ufDE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"osm4Zmm4KS5w","executionInfo":{"status":"ok","timestamp":1644272123528,"user_tz":360,"elapsed":7597,"user":{"displayName":"Yuan Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBJ0riEbMZ4_ksWQUJjjNFSi7ruMJ8UYx9uTtT=s64","userId":"11796422207038387991"}},"outputId":"bec13fbc-605a-45fd-b2ec-4837b318e320"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scrapy in /usr/local/lib/python3.7/dist-packages (2.5.1)\n","Requirement already satisfied: zope.interface>=4.1.3 in /usr/local/lib/python3.7/dist-packages (from scrapy) (5.4.0)\n","Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.7/dist-packages (from scrapy) (2.0.5)\n","Requirement already satisfied: pyOpenSSL>=16.2.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (22.0.0)\n","Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.7/dist-packages (from scrapy) (0.1.16)\n","Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (0.4.0)\n","Requirement already satisfied: h2<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (3.2.0)\n","Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (1.22.0)\n","Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (36.0.1)\n","Requirement already satisfied: Twisted[http2]>=17.9.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (22.1.0)\n","Requirement already satisfied: service-identity>=16.0.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (21.1.0)\n","Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from scrapy) (1.1.0)\n","Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (1.6.0)\n","Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from scrapy) (1.6.2)\n","Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scrapy) (1.0.4)\n","Requirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (4.2.6)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.0->scrapy) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.0->scrapy) (2.21)\n","Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from h2<4.0,>=3.0->scrapy) (5.2.0)\n","Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.7/dist-packages (from h2<4.0,>=3.0->scrapy) (3.0.0)\n","Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.7/dist-packages (from itemloaders>=1.0.1->scrapy) (0.10.0)\n","Requirement already satisfied: six>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from parsel>=1.5.0->scrapy) (1.15.0)\n","Requirement already satisfied: pyasn1 in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n","Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy) (21.4.0)\n","Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from Twisted[http2]>=17.9.0->scrapy) (20.2.0)\n","Requirement already satisfied: incremental>=21.3.0 in /usr/local/lib/python3.7/dist-packages (from Twisted[http2]>=17.9.0->scrapy) (21.3.0)\n","Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.7/dist-packages (from Twisted[http2]>=17.9.0->scrapy) (15.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted[http2]>=17.9.0->scrapy) (3.10.0.2)\n","Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.7/dist-packages (from Twisted[http2]>=17.9.0->scrapy) (21.0.0)\n","Requirement already satisfied: priority<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from Twisted[http2]>=17.9.0->scrapy) (1.3.0)\n","Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted[http2]>=17.9.0->scrapy) (2.10)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zope.interface>=4.1.3->scrapy) (57.4.0)\n","Collecting praw\n","  Downloading praw-7.5.0-py3-none-any.whl (176 kB)\n","\u001b[K     |████████████████████████████████| 176 kB 5.0 MB/s \n","\u001b[?25hCollecting update-checker>=0.18\n","  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n","Collecting websocket-client>=0.54.0\n","  Downloading websocket_client-1.2.3-py3-none-any.whl (53 kB)\n","\u001b[K     |████████████████████████████████| 53 kB 2.0 MB/s \n","\u001b[?25hCollecting prawcore<3,>=2.1\n","  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\n","Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from prawcore<3,>=2.1->praw) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2021.10.8)\n","Installing collected packages: websocket-client, update-checker, prawcore, praw\n","Successfully installed praw-7.5.0 prawcore-2.3.0 update-checker-0.18.0 websocket-client-1.2.3\n"]}],"source":["!pip install scrapy\n","!pip install praw"]},{"cell_type":"code","source":["import requests\n","import requests.auth\n","\n","client_auth = requests.auth.HTTPBasicAuth('PkK2HAiKXWi4lVmek7mu9g','G-vjkM8dq1NuZjwizkEd3UMLgHfhfg')\n","post_data = {\n","      \"grant_type\": \"password\", \"username\": \"cysbc1999\", \"password\": \"cysbc1999\"}\n","headers = {\n","      \"User-Agent\": \"ChangeMeClient/0.1 by YourUsername\"}\n","response = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth, data=post_data, headers=headers)\n","print(response.json())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LnIq0aJpKTz5","executionInfo":{"status":"ok","timestamp":1644271801518,"user_tz":360,"elapsed":861,"user":{"displayName":"Yuan Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBJ0riEbMZ4_ksWQUJjjNFSi7ruMJ8UYx9uTtT=s64","userId":"11796422207038387991"}},"outputId":"658e8151-a88a-47f4-f917-9ba6de113237"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["{'access_token': '116991958771-OKFo7l6gm28Iwkvrap8celUBEdddJw', 'token_type': 'bearer', 'expires_in': 3600, 'scope': '*'}\n"]}]},{"cell_type":"code","source":["import scrapy\n","import praw\n","import datetime\n","class RedditSpider(scrapy.Spider):\n","\tname = \"reddit\"\n","\tallowed_domains = [\"reddit.com\"]\n","\tstart_urls = [\n","\t\t\"https://www.reddit.com\"\n","\t]\n","\n","\tdef parse(self, response):\n","        #使用client id 和secret 进行登陆\n","\t\treddit = praw.Reddit(client_id='PkK2HAiKXWi4lVmek7mu9g', client_secret='G-vjkM8dq1NuZjwizkEd3UMLgHfhfg',\n","\t\t\tgrant_type='client_credentials', user_agent='mytestscripts/1.0')\n","\t\t\n","\t\t\"\"\"\n","\t\tsub = reddit.submission(id='9klf7s')\n","\t\t#print(sub.title)\n","\t\t#pprint.pprint(vars(sub))\n","\t\t\"\"\"\n","\t\t#可以通过 subreddit.stream.submissions()来监控某一个子版块出现的新帖子\n","\t\t#subreddit = reddit.subreddit('dapps')\n","\t\t#for sub in subreddit.stream.submissions():\n","        \n","        #limit=None来获取所有的贴子，默认为100\n","        #每次得到的属性类别数量可能不一样\n","\t\tsubs = reddit.subreddit('dapps').new(limit=None)\n","\t\tfor sub in subs:\n","\t\t\titem = RedditItem()\n","\n","\t\t\titem['html'] = response.body\n","\t\t\t#print(item['html'])\n","\n","            #permalink是网站下该帖子的前缀，需要和网站地址拼接构成该帖子的链接地址\n","\t\t\turl = 'https://{}{}'.format(self.allowed_domains[0], sub.permalink)\n","\t\t\titem['url'] = url\n","\t\t\tredditor = sub.author\n","            #作者可能为空\n","\t\t\t#print(\"author:\", redditor.name)\n","\t\t\tif redditor is not None:\n","\t\t\t\titem['author'] = redditor.name\n","\t\t\telse:\n","\t\t\t\titem['author'] = \"\"\n","\n","            #sub.created_utc是一个utc时间戳，需要转换成datetime格式\n","\t\t\t#print(\"created utc:\", sub.created_utc)\n","\t\t\titem['created_time'] = datetime.datetime.utcfromtimestamp(sub.created_utc)\n","\n","            #如果帖子本身只是一个超链接，那么sub.selftext为空\n","\t\t\titem['selftext'] = sub.selftext\n","\t\t\tif sub.is_self==False :\n","\t\t\t\titem['selftext'] = sub.url\n","\n","\t\t\tyield item\n","\n","\n","html_insert = '''insert into reddit_dapps_html(html) values('{html}')'''\n","reddit_insert = '''insert into reddit_dapps(url, url_md5, title, author, \n","created_time, selftext, score, num_comments, upvote_ratio)\n","          values('{url}', '{url_md5}', '{title}', '{author}', \n","          '{created_time}', '{selftext}', '{score}', '{num_comments}', '{upvote_ratio}')'''\n","\n","def process_item(self, item, spider):\n","  html = item['html']\n","  if html:\n","    item['html'] = html.strip().decode(encoding=\"utf-8\")\n","  ......\n","\n","  #将时间格式化\n","  #created_time\n","  created_time = item['created_time']\n","  if created_time:\n","    item['created_time'] = created_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n","\n","  selftext = item['selftext']\n","  if selftext:\n","    item['selftext'] = selftext.replace('\\n', '').replace('  ', ' ')\n","\n","  sqltext1 = self.html_insert.format(\n","    html = pymysql.escape_string(item['html']))\n","\n","  #由于score等是数字，需要先转换为字符串格式\n","  sqltext2 = self.reddit_insert.format(\n","    url = pymysql.escape_string(item['url']),\n","    score = pymysql.escape_string(str(item['score'])),\n","    num_comments = pymysql.escape_string(str(item['num_comments'])),\n","    upvote_ratio = pymysql.escape_string(str(item['upvote_ratio'])))\n","  self.cursor.execute(sqltext1)\n","  self.cursor.execute(sqltext2)\n","\n","  return item\n","\n","\n","def open_spider(self, spider):\n","    # connet database\n","      # 选择字符集为'utf8mb4'\n","    self.connect = pymysql.connect(\n","        host=self.settings.get('MYSQL_HOST'),\n","        port=self.settings.get('MYSQL_PORT'),\n","        db=self.settings.get('MYSQL_DBNAME'),\n","        user=self.settings.get('MYSQL_USER'),\n","        passwd=self.settings.get('MYSQL_PASSWD'),\n","        charset='utf8mb4',\n","        use_unicode=True)"],"metadata":{"id":"btYClf2qPLki","executionInfo":{"status":"ok","timestamp":1644272694573,"user_tz":360,"elapsed":764,"user":{"displayName":"Yuan Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBJ0riEbMZ4_ksWQUJjjNFSi7ruMJ8UYx9uTtT=s64","userId":"11796422207038387991"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"DxDFtQi6SloO"},"execution_count":null,"outputs":[]}]}